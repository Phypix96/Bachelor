\documentclass[10pt,a4paper]{report}
\usepackage{geometry}
\geometry{
  left=3.5cm,
  right=2.5cm,
  top=2cm,
  bottom=4cm,
  bindingoffset=5mm
}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{bm}
\usepackage{bbold}

\newcommand{\SumIndex}{\sigma_1,\ldots,\sigma_L}
\newcommand{\SumIndexTau}{\tau_1,\ldots,\tau_L}

\author{Philipp Haim}
\title{Bachlor Arbeit}


\begin{document}
\maketitle
\tableofcontents

\chapter{Matrix-Produkt Zustände}\label{MPS_chapter}
Man betrachte ein System aus $L$ Teilchen an festen Positionen. Jedes dieser Teilchen habe einen lokalen Konfigurationsraum mit Dimension $d$. Es kann nun jeder Zustand geschrieben werden als 
\begin{equation}\label{psi_equ}
|\psi\rangle=\sum_{\SumIndex} c_{\SumIndex}|\SumIndex\rangle
\end{equation}
Die Anzahl der nötigen Koeffizienten $c_{\sigma_1,\ldots,\sigma_L}$ steigt dabei mit $d^L$ an, also exponentiell mit der Anzahl der Teilchen in dem System. Für große Systeme macht dies Rechnungen mit den Zuständen über die Koeffizienten unöglich. Wie wir in Abschnitt \ref{compress} sehen werden, kann die Anzahl der benötigten Koeffizenten effizent reduziert werden, wenn man die Koeffizienten als Produkt von $L$ Matrizen darstellt.\cite{MPS-main}
\begin{equation}\label{MPS_form_equ}
c_{\SumIndex}=A^{\sigma_1}A^{\sigma_2}\ldots A^{\sigma_{L-1}}A^{\sigma_L}
\end{equation}
Man spricht hierbei von einem Matrix-Produkt Zustand (MPS).\footnote{Abkürzung der englischen Bezeichung \textit{Matrix-Product State}}
Es soll daher zunächst gezeigt werden, wie ein allgemeiner Zustand in die Form \ref{MPS_form_equ} gebracht werden kann.
\section{Konstruktion eines MPS}\label{construction}


Dafür werden, wie in Gleichung \ref{MPS_constr_equ} ausgeführt, die Koeffizienten in einen $(d\times d^{L-1})$ Vektor umgeschrieben Dieser wird anschließend mit einer Singulärwertzerlegung in das Produkt dreier Matrizen $USV^{\prime}$ aufgeteilt. Zuletzt wird die $(d\times d)$ Matrix $U$ in $d$ $(1\times d)$ Matrizen $A^{\sigma_i}$ aufgeteilt. 
\begin{equation}
\label{MPS_constr_equ}
\begin{split}
c_{\SumIndex}&=\Psi_{\sigma_1,(\sigma_2,\ldots,\sigma_L)} =\sum_{a_1}U_{\sigma_1,a_1}S_{a_1,a_1}V^\dagger_{a_1,(\sigma_2,\ldots,\sigma_L)} \\
&\equiv\sum_{a_1}U_{\sigma_1,a_1}\tilde{c}_{a_1,(\sigma_2,\ldots,\sigma_L)}=\sum_{a_1}A_{1,a_1}^{\sigma_1}\tilde{c}_{a_1,\sigma_2,\ldots,\sigma_L} \\
\end{split}
\end{equation}
Im nächsten Schritt wird $\tilde{c}$ zu einer $(d^2 \times d^{L-2})$ Matrix umgeformt und erneut wie im ersten Schritt zerlegt. Dies wird wiederholt, bis nur noch eine $(d\times d)$ Matrix übrig bleibt, welche in die $d$ $A^{\sigma_L}$ Matrizen zerlegt wird.
\begin{equation}
\label{MPS_fin_equ}
\begin{split}
c_{\SumIndex}&=\sum_{a_1}\sum_{a_2}A_{1,a_1}^{\sigma_1}U_{(a_1 \sigma_2),a_2}S_{a_2,a_2}V_{a_2,(\sigma_3,\ldots,\sigma_L)}^\dagger\\
&=\sum_{a_1}\sum_{a_2}A_{1,a_1}^{\sigma_1}A_{a_1,a_2}^{\sigma_2}\Psi_{(a_2,\sigma_3),(\sigma_4,\ldots,\sigma_L)}\\
&=\ldots=\sum_{a_1,\ldots,a_{L-1}}A_{1,a_1}^{\sigma_1}A_{a_1,a_2}^{\sigma_2}\ldots A_{a_{L-2},a_{L-1}}^{\sigma_{L-1}}A_{a_{L-1},1}^{\sigma_L}
\end{split}
\end{equation}

Es ist anzumerken, dass es sich bei den Matrizen $A^{\sigma_i}$ und $A^{\sigma_j}$ für $i\neq j$ um unterschiedliche Matrizen handelt. Um die Notation einfach zu halten, wird auf eine explizite Unterscheidung allerdings verzichtet.

Da die in Gleichung \ref{MPS_fin_equ} auftretenden Summen als Matrixmultiplikationen aufgefasst werden können, liegen die Koeffizienten nun in der Form aus Gleichung \ref{MPS_form_equ} vor. 

Die Matrizen $A$ erfüllen aufgrund ihrer Konstruktion einige spezielle Eigenschaften. Da die $U$ Matrizen der Singulärwertzerlegung links-normalisiert sind\footnote{D.h. $U^\dagger U=1$}, gilt für alle Matrizen $A^{\sigma_i}$
\begin{equation}
\sum_{\sigma_i}A^{\sigma_i \dagger}A^{\sigma_i}=1
\end{equation}

Weiters ist die Dimension der einzelnen Matrizen durch ihre Konstruktion fix bestimmt. Diese sind für $A^{\sigma_1}\ldots A^{\sigma_L}$ gegeben durch $(1\times d)(d\times d^2)\ldots(d^2\times d)(d\times 1)$. Für eine gerade Anzahl an Teilchen haben die beiden innersten Matrizen Dimension $(d^{\frac{L}{2}-1}\times d^{\frac{L}{2}})(d^{\frac{L}{2}}\times d^{\frac{L}{2}-1)}$, während für eine ungerade Anzahl die innerste Matrix Dimension $(d^{\frac{L}{2}-1}\times d^{\frac{L}{2}-1})$ hat.\\

Es ist anzumerken, dass dies nur eine mögliche Konstruktion eines MPS ist. Alternativ kann die Zerlegung von $c$ auch bei dem letzten Koeffizienten begonnen werden. Man spricht von einem rechts-kanonischen MPS. Wenn bis zu einer Position $l<L$ eine links-kanonische, und von da an rechts-kanonische Zerlegung vorgenommen wird, ist von einem gemischt-kanonischen MPS die Rede. In diesem Fall erhält man einen MPS der Form
\begin{equation}
|\psi\rangle=\sum_{\SumIndex}A^{\sigma_1}\ldots A^{\sigma_l}S B^{\sigma_{l+1}}\ldots B^{\sigma_L}|\bm{\sigma}\rangle
\end{equation}
$S$ ist dabei eine Diagonalmatrix, alle A Matrizen sind links-orthogonal und alle B Matrizen rechts-orthogonal.

Aus der gemischt-kanonischen Zerlegung kann einfach eine Schmidt-Zerlegung des Zustandes konstruiert werden. Dazu führt man folgende neue Vektoren ein:
\begin{equation}
\begin{split}
|a_l\rangle_A&=\sum_{\SumIndex}(A^{\sigma_1}\ldots A^{\sigma_l})_{1,a_l}|\sigma_1\ldots\sigma_l\rangle \\
|a_l\rangle_B&=\sum_{\SumIndex}(B^{\sigma_{l+1}}\ldots B^{\sigma_L})_{a_l,1}|\sigma_{l+1}\ldots\sigma_L\rangle\\
\end{split}
\end{equation}
Da diese Vektoren aufgrund ihrer Konstruktion eine Orthogonalbasis bilden, lässt sich der Zustand $|\psi\rangle$ schreiben als
\begin{equation}\label{MPS_Schmidt_equ}
|\psi\rangle=\sum_{a_l}S_{a_l,a_l}|a_l\rangle_A|a_l\rangle_B
\end{equation}

\section{Kompression eines MPS}\label{compress}
Die Zerlegung eines Zustands in einen MPS bringt im Allgemeinen noch keinen numerischen Vorteil, da die Matrizengröße erneut exponentiell steigt. Es ist also nötig, die Dimension der Matrizen signifikant zu reduzieren, und dabei einen möglichst kleinen Fehler in den Koeffizienten $c$ zu machen. Der einfachste Weg, dies zu machen führt erneut über die Singulärwertzerlegung.\\


Wir betrachten einen gemischt-kanonischen MPS mit der Diagonalmatrix $S$ an Position $l$. Wie in Abschnitt \ref{construction} gezeigt, lässt sich aus dieser Form die Schmidt-Zerlegung $|\psi\rangle=\sum_{a_l=1}^{D^{\prime}} S_{a_l,a_l}|a_l\rangle_A|a_l\rangle_B$ des Zustandes ablesen. Dabei ist $D^\prime$ die Dimension der Diagonalmatrix. Ziel ist es nun, diese Dimension auf $D<D^\prime$ zu reduzieren, während die 2-Norm des Zustandes möglichst unverändert bleiben soll.
Die einfachste Art dies zu erreichen ist, die Summe auf die $D$ größten Einträge zu beschränken.

%Wie in \todo{Fehler von Beschränkung der Eigenwerte auf Seite 114, mit Quelle!} gezeigt, kann dies erreicht werden, in dem die Summe auf die $D$ größten Einträge in $S$ beschränkt wird. 
%Der sich dabei ergebende Fehler ist dabei beschränkt durch
%\begin{equation}
%||\,|\psi\rangle-|\psi_{trunc}\rangle||_2^2\leq 2\sum_{i=D^\prime+1}^D S_{i,i}^2
%\end{equation}

Diese Kompression kann einfach auf den MPS Formalismus übertragen werden, indem von der Matrix $A^{\sigma_l}$ nur die ersten $D$ Spalten, von $B^{\sigma_{l+1}}$ die ersten $D$ Zeilen und von $S$ die ersten $D$ Zeilen und Spalten behalten werden. Es ist zu beachten, dass dabei die Norm des Zustandes nicht erhalten bleibt.\\

Um diese Dimensionsreduktion für die Matrix $A^{sigma_l}$ durchzuführen, muss der Zustand in einer Schmidt-Zerlegung vorliegen. Um für den gesamten MPS die maximale Dimension der Matrizen auf $D$ zu reduzieren, muss diese daher für jedes $l$ diese Zerlegung vorgenommen werden (vgl. Gleichung \ref{MPS_Schmidt_equ}). Wir nehmen im Folgenden an, es liegt ein links-kanonischer MPS vor.\footnote{Dies stellt kein Einschränkung da, da jeder Zustand in einen links-kanonischen umgeschrieben werde kann \cite{page=129,MPS-main}}\todo{Fußnote zitieren}. Zunächst werden die letzten Matrizen $A^{\sigma_L}$ zu einer Matrix zusammengefasst, indem der Index $\sigma_L$ mit dem Spalten-Index zusammengefasst wird.

\begin{equation}
A_{a_{i-1},a_i}^{\sigma_i}\rightarrow A_{a_{i-1},(\sigma_i,a_i)}
\end{equation}

Diese Matrix wird nun einer Singulärwertzerlegung unterzogen, sodass $A=USV^\dagger$. $V^\dagger$ lässt sich nun erneut in $d$ rechts-orthogonale Matrizen $B^{\sigma_L}$ aufteilen. Der Zustand ließt sich nun als
\begin{equation}
|\psi\rangle=\sum_{\bm{\sigma}}A^{\sigma_1}\ldots A^{\sigma_{L-1}}USB^{\sigma_L}|\bm{\sigma}\rangle
\end{equation}
Es können nun die Matrizen $U$, $S$ und $B^{\sigma_L}$, wie zuvor beschrieben, zu $\tilde{U}$, $\tilde{S}$ und $\tilde{B}^{\sigma_L}$ trunkiert werden. Anschließend wird $A^{\sigma_{L-1}}\tilde{U}\tilde{S}$ zu einer neuen Matrix $M^{\sigma_{L-1}}$ ausmultipliziert. Es kann nun derselbe Vorgang an der Position $L-1$ durchgeführt werden. Auf diese Art kann jede  Matrix auf eine maximale Dimension von $D$ reduziert werden.
\begin{equation}
\begin{split}
|\psi\rangle&\rightarrow \sum_{\bm{\sigma}}A^{\sigma_1}\ldots A^{\sigma_{L-2}}M^{\sigma_{L-1}}\tilde{B}^{\sigma_L}|\bm{\sigma}\rangle\rightarrow \sum_{\bm{\sigma}}A^{\sigma_1}\ldots (A^{\sigma_{L-2}}\tilde{U}\tilde{S})\tilde{B}^{\sigma_{L-1}}\tilde{B}^{\sigma_L}|\bm{\sigma}\rangle\rightarrow\\
&\rightarrow \ldots=\sum_{\bm{\sigma}}\tilde{B}^{\sigma_1}\ldots \tilde{B}^{\sigma_{L}}|\bm{\sigma}\rangle
\end{split}
\end{equation}

In \cite{MPS_error} wurde gezeigt, dass der bei dieser Kompression auftretende Fehler beschränkt ist durch
\begin{equation}
||\,|\psi\rangle-|\psi_{trunc}\rangle||_2^2\leq 2\sum_{i=1}^{L}\epsilon_i(D)
\end{equation}
Dabei ist $\epsilon_i$ die Summe über alle bei der Näherung ignorierten Eigenwerte des reduzierten Dichteoperators. Dies ist aber gerade die Quadratsumme über alle ignorierten Singulärwerte.Dieser Fehler ist also klein, wenn das Eigenwert-Spektrum des reduzierten Dichteoperators schnell genug abfällt, um ab dem Element D keinen signifikanten Betrag zu leisten. \todo{Wann gilt das? Zitat!}

Dieser Algorithmus kann auch von einem rechts-kanonischen Zustand aus begonnen werden. Dafür werden die Matrizen von links nach rechts komprimiert, und die einzelnen Matrizen $B^{\sigma_i}$ werden über den Zeilenindex zusammen gefasst.

\begin{equation}
B_{a_{i-1},a_{i}}^{\sigma_i}\rightarrow B_{(\sigma_i a_{i-1}),a_i}
\end{equation}

Diese Kompression ist nicht optimal und kann für $D\ll D^\prime$ sehr langsam werden. Weiters ist es im allgemeinen nötig, den MPS zunächst in eine kanonische Form zu bringen. Es ist alternativ auch möglich, den MPS durch eine iterative Suche nach der trunkierten Matrix zu komprimieren, der den 2-Norm Abstand zu dem ursprünglichen MPS minimiert. Während diese Methode zwar optimal ist, stellt auch hier Geschwindigkeit ein Problem dar, wenn der erste Iterationsschritt zufällig gewählt wird.

\section{Erwartungswerte und Überläppe}\label{MPO}
Um den Überlapp von zwei Zuständen $|\phi\rangle$ und $|\psi\rangle$ zu berechnen, ist es zuerst nötig, den dualen Zustand durch Matrizen darstellen zu können. Aus der Zerlegung der Koeffizienten in Matrix-Produkte folgt dann
\begin{equation}
\begin{split}
\langle\phi|&=\sum_{\SumIndex}c_{\SumIndex}^{\ast}\langle\SumIndex|=\sum_{\SumIndex} A^{\sigma_1\ast}\ldots A^{\sigma_L \ast}\langle\SumIndex|\\
 &= \sum_{\SumIndex}A^{\sigma_L\dagger}\ldots A^{\sigma_1 \dagger}\langle\SumIndex|
\end{split}
\end{equation}
Damit ergibt sich $\langle\phi|\psi\rangle$ zu 
\begin{equation}
\langle\phi|\psi\rangle=\sum_{\bm{\sigma}}\tilde{A}^{\sigma_L \dagger}\ldots\tilde{A}^{\sigma_1 \dagger} A^{\sigma_1}\ldots A^{\sigma_L} 
\end{equation}
Das direkte Berechnen dieser Summe ist für große Systeme in der Praxis nicht möglich. Allerdings kann der Rechenaufwand drastisch reduziert werden, indem die Reihenfolge der Summen umsortiert wird. Da nach dem Multiplizieren von $\tilde{A}^{\sigma_1\dagger}$ und $A^{\sigma_1}$ keiner der Terme mehr von $\sigma_1$ abhängt, kann diese Summe daher vorgezogen werden. Da dasselbe im Anschluss für $\sigma_2$ usw. gemacht werden kann, ergibt sich für den Überlapp
\begin{equation}
\langle\phi|\psi\rangle=\sum_{\sigma_L}\tilde{A}^{\sigma_L\dagger}(\ldots(\sum_{\sigma_2}\tilde{A}^{\sigma_2\dagger}(\sum_{\sigma_1}\tilde{A}^{\sigma_1\dagger}A^{\sigma_1})A^{\sigma_2})\ldots)A^{\sigma_L}
\end{equation}
Auf dies Art kann der Rechenaufwand dramatisch reduziert werden.\\

Um Erwartungswerte berechnen zu können, ist es weiters nötig, die Wirkung eines Operators auf einen MPS zu untersuchen. Dafür ist eine Beschreibung des Operators im MPS Formalismus nötig, es wird von einem Matrix-Produkt Operator (MPO) gesprochen. Die Koeffizienten eines MPO können erneut als Produkt von Matrizen geschrieben werden \todo{Quellen aus Schollwöck nachschlagen}. Damit ergibt sich für den Operator
\begin{equation}
\hat{O}=\sum_{\bm{\sigma},\bm{\sigma^\prime}}W^{\sigma_1,\sigma_1^\prime}\ldots W^{\sigma_L,\sigma_L^\prime} |\bm{\sigma}\rangle\langle\bm{\sigma^\prime}|
\end{equation}

Im Folgenden wollen wir den Effekt eines Operators auf einen MPS untersuchen. Ziel ist es, das Ergebnis erneut als MPS darstellen zu können, da dies das einfache Berechnen von Erwartungswerten ermöglicht.

\begin{equation}\label{MPO_MPS_equ}
\begin{split}
\hat{O}|\phi\rangle & = \sum_{\bm{\sigma},\bm{\sigma^\prime}}(W^{\sigma_1,\sigma_1^\prime}\ldots W^{\sigma_L,\sigma_L^\prime})(A^{\sigma_1}\ldots A^{\sigma_L})|\bm{\sigma}\rangle \\
& =\sum_{\bm{\sigma},\bm{\sigma^\prime}}\sum_{\bm{a},\bm{b}}(W_{1,b_1}^{\sigma_1,\sigma_1^\prime}A_{1,a_1}^{\sigma_1^\prime})(W_{b_1,b_2}^{\sigma_2,\sigma_2^\prime}A_{a_1,a_2}^{\sigma_2^\prime})\ldots(W_{b_{L-1},1}^{\sigma_L,\sigma_L^\prime}A_{a_{L-1},1}^{\sigma_L^\prime})|\bm{\sigma}\rangle \\
&=\sum_{\bm{\sigma}}\sum_{\bm{a},\bm{b}}N_{(1,1),(b_1,a_1)}^{\sigma_1}N_{(b_1,a_1),(b_2,a_2)}^{\sigma_2}\ldots N_{(b_{L-1},a_{L-1}),(1,1)}^{\sigma_L}|\bm{\sigma}\rangle \\
\end{split}
\end{equation}
Es lässt sich im letzten Schritt erneut die MPS-Form des ursprünglichen Zustands erkennen. Die Größe der Matrizen ist allerdings um die der MPO-Matrizen gestiegen. Zusammenfassend wird aus einer Matrix $A^{\sigma_i}$ unter Anwendung eines MPO eine neue Matrix $N^{\sigma_i}$ wie folgt:

\begin{equation}
N_{(b_{i-1},a_{i-1}),(b_i,a_i)}^{\sigma_i}=\sum_{\bm{\sigma_i^\prime}}W_{b_{i-1},b_i}^{\sigma_i\sigma_i^\prime}A_{a_{i-1},a_i}^{\sigma_i^\prime}
\end{equation}
Damit ergibt sich der Erwartungswert eines Operators zu
\begin{equation}
\langle\phi|\hat{O}|\psi\rangle=\sum_{\bm{\sigma}}\tilde{A}^{\sigma_L \dagger}\ldots\tilde{A}^{\sigma_1 \dagger} N^{\sigma_1}\ldots N^{\sigma_L}\\
\end{equation}

\section{Zeitentwicklung}\label{tMPS}

Um die Dynamik eines MPS berechnen zu können, ist eine Beschreibung des Zeitentwicklungs-Operators $e^{-it\hat{H}t}$ als MPO nötig. Dies erfordert, dass der gesamte Operator als Matrix-Produkt geschrieben wird, in dem die Wirkung auf die einzelnen Gitterplätze völlig faktorisiert ist, also eine Matrix nur auf eine Position einwirkt. Im folgenden werden nur Interaktionen zwischen benachbarten Gitterplätzen behandelt, auch wenn weiter reichende Interaktionen prinzipiell beschrieben werden können.

Führt man $\hat{h}_{i,i+1}$ als Wechselwirkungs-Hamiltonian zwischen den Teilchen an Positionen $i$ und $i+1$ ein, ergibt sich für den Gesamt-Hamiltionan
\begin{equation}
\hat{H}=\sum_{i=1}^{L-i}\hat{h}_{i,i+1}
\end{equation}

Im Allgemeinen hat dieser Operator Dimension $d^L$ und kann daher nicht exakt berechnet werden. Betrachtet man nun einen kleinen Zeitschritt $\tau$, so kann mittels einer Trotter-Zerlegung 1. Ordnung der Zeitentwicklungs-Operator geschrieben werden als\todo{Anhang Trotter-Zerlegung bis 2.Ordnung}

\begin{equation}\label{Trotter_equ}
e^{-i\hat{H}\tau}= e^{-i\hat{h}_{1,2}\tau}e^{-i\hat{h}_{2,3}\tau}\ldots e^{-i\hat{h}_{L-1,L}\tau}+O(\tau^2)
\end{equation}

Der Fehler dieser Zerlegung rührt daher, dass die einzelnen Terme im Allgemeinen nicht kommutieren. Ein größeren Zeitschritt kann nun erreicht werden, indem mehrere dieser kurzen Schritte durchgeführt werden.

Da in Gleichung \ref{Trotter_equ} jeder zweite Term kommutiert,\footnote{Sind Terme aus Gleichung \ref{Trotter_equ} nicht benachbart, wirken sie nie auf dasselbe Teilchen ein, und wirken daher stets auf einen anderen Teil des Hilbert-Raums} kann diese in folgende Form umgeschrieben werden
\begin{equation}
\begin{split}
e^{-i\hat{H}\tau}&=(e^{-i\hat{h}_{1,2}\tau}e^{-i\hat{h}_{3,4}\tau}\ldots)(e^{-i\hat{h}_{2,3}\tau}e^{-i\hat{h}_{4,5}\tau}\ldots)+O(\tau^2)\\
&\equiv e^{-i\hat{H}_{\text{odd}}\tau}e^{-i\hat{H}_{\text{even}}\tau}+O(\tau^2)
\end{split}
\end{equation}

Diese Abschätzung lässt sich weiter verbessern, indem eine Trotter-Zerlegung 2. Ordnung durchgeführt wird. Diese nimmt folgende Form an
\begin{equation}
e^{-i\hat{H}\tau}=e^{-i\hat{H}_{\text{odd}}\tau/2}e^{-i\hat{H}_{\text{even}}\tau}e^{-i\hat{H}_{\text{odd}}\tau/2}+O(\tau^3)
\end{equation}

Diese Näherung kann weiter verbessert werden, indem höhere Ordnungen der Zerlegung verwendet werden, oder andere, weniger symmetrische Algorithmen.

Führt man nun mehrere dieser Zeitschritte hintereinander aus, kann der benötigte Rechenaufwand signifikant reduziert werden, indem ausgenutzt wird, dass $\hat{H}_{\text{odd}}$ mit sich selbst kommutiert. Dadurch können der letzte und der erste Term zweier aufeinanderfolgender Trotter-Zerlegungen 2. Ordnung zu einem zusammengefasst werden ohne an Genauigkeit zu verlieren.\par

In der aktuellen Form kann der Zeitentwicklungs-Operator jedoch noch nicht auf einen MPS angewandt werden. Dazu ist es zunächst nötig, diesen in einen MPO umzuformen, bei dem ein Operator nur auf eine Position wirkt. Dafür soll zunächst o.B.d.A. der Operator $e^{-i\hat{h}_{1,2}\tau}\equiv O$ betrachtet werden. Ziel ist es, diesen zu einem Produkt von 1-Teilchen Operatoren zu faktorisieren. Dafür wird zunächst die Basis des Operators $O$ von $(\sigma_1\sigma_2,\sigma_1^{\prime}\sigma_2^{\prime})$ zu $(\sigma_1\sigma_1^{\prime},\sigma_2\sigma_2^{\prime})$ umsortiert. Um dies in ein Matrix-Produkt zu zerlegen wird anschließend noch eine Singulärwert-Zerlegung durchgeführt. 

\begin{equation}
O^{\sigma_1 \sigma_2,\sigma_1^\prime \sigma_2^\prime}=P_{(\sigma_1 \sigma_1^\prime),(\sigma_2 \sigma_2^\prime)}=\sum_k U_{(\sigma_1 \sigma_1^\prime),k}S_{k,k}V_{k,(\sigma_2 \sigma_2^\prime)}^\dagger=\sum_k M_{1,k}^{\sigma_1 \sigma_1^\prime} \bar{M}_{k,1}^{\sigma_2 \sigma_2^\prime}
\end{equation}

Es wurden hier die neuen Matrizen $M^{\sigma_1,\sigma_1^{\prime}}$ und $\bar{M}^{\sigma_2,\sigma_2^{\prime}}$ eingeführt. Diese sind definiert als $M_{1,k}^{\sigma_1,\sigma_1^{\prime}}=U_{(\sigma_1 \sigma_1^\prime),k}\sqrt{S_{k,k}}$ und $\bar{M}_{k,1}^{\sigma_2,\sigma_2^{\prime}}=\sqrt{S_{k,k}}V_{k,(\sigma_2 \sigma_2^\prime)}^\dagger$. Führt man diese Faktorisierung nun für jeden Operator in $e^{-i\hat{H}_{\text{odd}}\tau}$ und $e^{-i\hat{H}_{\text{even}}\tau}$ durch, erhält man\footnote{Die Identität ist hierbei gegeben als $\mathbb{1}_{1,1}^{\sigma,\sigma^{\prime}}=\delta_{\sigma,\sigma^{\prime}}$}
\begin{equation}
\begin{split}
(e^{-i\hat{H}_{\text{odd}}\tau})^{\bm{\sigma},\bm{\sigma^\prime}}&=\sum_{\textbf{k}}M_{1,k_1}^{\sigma_1 \sigma_1^\prime} \bar{M}_{k_1,1}^{\sigma_2 \sigma_2^\prime}M_{1,k_2}^{\sigma_3 \sigma_3^\prime} \bar{M}_{k_2,1}^{\sigma_4 \sigma_4^\prime}\ldots\\
(e^{-i\hat{H}_{\text{even}}\tau})^{\bm{\sigma},\bm{\sigma^\prime}}&=\sum_{\textbf{k}}\mathbb{1}_{1,1}^{\sigma_1 \sigma_1^\prime} M_{1,k_1}^{\sigma_2 \sigma_2^\prime}\bar{M}_{k_1,1}^{\sigma_3 \sigma_3^\prime} M_{1,k_2}^{\sigma_4 \sigma_4^\prime}\bar{M}_{k_2,1}^{\sigma_5 \sigma_5^\prime}\ldots\\
\end{split}
\end{equation}
Dieser MPO kann nun, wie in Abschnitt \ref{MPO} erläutert, auf den MPS angewandt werden.

\section{Stochastische Matrix-Produkt Zustände}
In in diesem Kapitel wurde bereits behandelt, wie ein quantenmechanischer Zustand in die Form eines MPS gebracht werden kann. Es ist allerdings auch möglich, eine klassische Wahrscheinlichkeitsverteilung $P(\SumIndex)$ derart zu zerlegen. Der zu dieser Verteilung gehörige Zustand $|p\rangle$ kann geschrieben werden als
\begin{equation}
|p\rangle=\sum_{\SumIndex}P(\SumIndex)|\SumIndex\rangle
\end{equation}

Da es sich um Wahrscheinlichkeiten handelt, normiert man diesen Zustand über die $\text{L}_1$-Norm, also $\sum_{\bm{\sigma}}P(\bm{\sigma})=1$. Ziel ist es nun, $P(\bm{\sigma})$ als Produkt von Matrizen zu schreiben, sodass
\begin{equation}
P(\bm{\sigma})=A^{\sigma_1}\ldots A^{\sigma_L}
\end{equation}

Um dieses Produkt weiterhin durch die $\text{L}_1$-Norm normieren zu können, muss von den Matrizen gefordert werden, dass sie elementweise positiv sind, also $A_{a_i,a_i+1}^{\sigma_{i+1}}\geq 0$. Man spricht in diesem Fall von einem stochastischen Matrix-Produkt Zustand (sMPS), da die Einträge der Matrizen als Wahrscheinlichkeiten aufgefasst werden können.
Versucht man nun allerdings, die Koeffizienten analog zu \ref{MPS_constr_equ} in Matrizen aufzuteilen, ist eben dies nicht der Fall, da eine Singulärwert-Zerlegung im Allgemeinen Matrizen mit gemisch positiv und negativen Einträgen erzeugen kann. Wird allerdings eine nicht-negative Matrix Faktorisierung (NMF) verwendet, können die Koeffizienten analog zu Gleichung \ref{MPS_constr_equ} in das Produkt von Matrizen aufgeteilt werden.\\

Zur Berechnung der Norm eines sMPS ist wie für einen MPS ein Umsortieren der Indizes von großem Vorteil. Es ergibt sich daher
\begin{equation}
\parallel|p\rangle\parallel_1=\sum_{\SumIndex}P(\SumIndex)=\sum_{\SumIndex}A^{\sigma_1}\ldots A^{\sigma_L}=\sum_{\sigma_1}A^{\sigma_1}(\ldots(\sum_{\sigma_L}A^{\sigma_L})\ldots)
\end{equation}

\todo{vmtl. FALSCH!!!!}Die Kompression eines sMPS kann analog zu einem normalen MPS durchgeführt werden, wenn die Singulärwert-Zerlegung durch eine NMF ersetzt wird,welche die Matrix in das Produkt von zwei Matrizen mit reduzierter Dimension zerlegt. Ebenso kann die Zeitentwicklung analog angewandt werden, wenn der Operator durch eine NMF zerlegt wird. Es ist allerdings zu beachten, dass sich das Verhalten des Fehlers bei der Kompression eines sMPS ändert, da dieser nicht mit der $L_2$, sondern der $L_1$-Norm normiert wird. Daraus folgt, dass 
\todo{Norm-Unterschiede}

%Zur Berechnung der Norm sowie von Erwartungswerten eines sMPS Zustands wird zunächst ein dualer Hilfszustand definiert
%\begin{equation}
%\langle\mathbf{I}|=\sum_{\SumIndex}\mathbb{1}^{\sigma_1}\ldots\mathbb{1}^{\sigma_L}\langle\SumIndex|
%\end{equation}
%Hierbei bezeichnet $\mathbb{1}^{\sigma_i}$ eine Einheitsmatrix, deren Zeilendimension mit der von $A^{\sigma_i}$ übereinstimmt. Es gilt dann
%\begin{equation}
%\begin{split}
%\parallel|p\rangle\parallel_1&=\sum_{\SumIndex}A^{\sigma_1}\ldots A^{\sigma_L}=\sum_{\sigma_1}A^{\sigma_1}(\ldots(\sum_{\sigma_L}A^{\sigma_L})\ldots)\\
%&=\sum_{\sigma_1}\mathbb{1}^{\sigma_1}A^{\sigma_1}(\ldots(\sum_{\sigma_L}\mathbb{1}^{\sigma_1}A^{\sigma_L})\ldots)=\langle\mathbf{I}|p\rangle
%\end{split}
%\end{equation}

Um den Erwartungswert eines Operators zu berechnen, muss dieser zunächst als MPO dargestellt werden. Die Anwendung dieses MPO's auf den Zustand erfolgt dann völlig analog zu Gleichung \ref{MPO_MPS_equ} und man erhält einen neuen Satz Matrizen $N^{\sigma_i}$. Mittels dieser kann $\langle\hat{O}\rangle$ wie folgt berechnet werden

\begin{equation}
\langle\hat{O}\rangle=\parallel \hat{O}|p\rangle\parallel_1=\sum_{\SumIndex}N^{\sigma_1}\ldots N^{\sigma_L}=\sum_{\sigma_1}N^{\sigma_1}(\ldots(\sum_{\sigma_L}N^{\sigma_L})\ldots)
\end{equation}




\chapter{Das ASEP-Modell}
Das ASEP-Modell (Asymmetric Simple Exclusion Process) beschreibt ein eindimensionales System mit offenen Randbedingungen. Es handelt sich dabei um ein Gitter mit $L$ Plätzen, die entweder einfach besetzt oder leer sind. Ist der erste Gitterplatz unbesetzt, so wird nach dem Zeitschritt $dt$ mit einer Wahrscheinlichkeit $\alpha dt$ ein Teilchen an dieser Position eingesetzt. Weiters wird in diesem Zeitschritt mit der Wahrscheinlichkeit $\beta dt$ ein Teilchen an der letzten Position aus dem Gitter entnommen, wenn diese besetzt ist. Zwischen den Gitterplätzen können sich die Teilchen nur in Richtung des letzten Gitterplatzes bewegen,\footnote{In diesem Fall wird von dem TASEP (totaly asysmmetric exclusion process) gesprochen} was sie mit der Wahrscheinlichkeit $pdt$ tun, vorausgesetzt der nächste Platz ist unbesetzt.\todo{Weitere Quelle finden} Da der einzige Einfluss des konkreten Wertes von $p$ eine Skalierung der Zeit ist, wird  dieser üblicherweise 1 gesetzt.\\

\section{Mathematische Beschreibung}
Die Dynamik des Systems kann zunächst durch Ratengleichungen beschrieben werden. Dafür sei $p_i(t+dt)$ die Wahrscheinlichkeit, dass die Position $i$ zum Zeitpunkt $t+dt$ besetzt ist und $\tau_i$ die Besetzungszahl zum Zeitpunkt $t$. Wir müssen hierbei drei Fälle unterscheiden:
\begin{equation}\label{rate_equ}
p_i(t+dt)=
\begin{cases}
\tau_1+[\alpha(1-\tau_1)-\tau_1(1-\tau_2)]dt & \text{für}\ i=1\\
\tau_i+[\tau_{i-1}(1-\tau_i)-\tau_i(1-\tau_{i+1})]dt & \text{für}\ 1<i<L\\
\tau_L+[\tau_{N-1}(1-\tau_N)-\beta\tau_N]dt & \text{für}\ i=L
\end{cases}
\end{equation}

Die Wahrscheinlichkeit, dass von dem System mit $L$ Gitterplätzen die spezielle Konfiguration $\{ \tau_1,\ldots,\tau_L \}$ angenommen wird, soll im Folgenden mit $P_L(\tau_1,\ldots,\tau_L)$ bezeichnet werden. Da es sich hier um kein abgeschlossenens System handelt, existiert kein Gleichgewichtszustand, es kann allerdings ein stationärer Zustand gefunden werden. Für diesen Zustand gilt

\begin{equation}
\frac{d}{dt}P_L(\tau_1,\ldots,\tau_L)=0
\end{equation}

Um diesen stationären Zustand zu finden muss zunächst 

\begin{equation}\label{P_umständlich}
\begin{split}
\frac{d}{dt}P_L(\tau_1,\ldots,\tau_L) = \sum_{i=1}^{L-1}((1-\tau_i)\tau_{i+1}-\tau_i(1-\tau_{i+1}) )P(\tau_1,\ldots,\sigma_i=1,\sigma_{i+1}=0,\ldots,\tau_L)\\
(\alpha\tau_1-\alpha(1-\tau_1)) P(0,\tau_2,\ldots,\tau_L)+(\beta(1-\tau_L)-\beta\tau_L) P(\tau_1,\tau_2,\ldots,1)
\end{split}
\end{equation}

Gleichung \ref{P_umständlich} lässt sich in eine übersichtlichere Form bringen, indem die auftretenden Koeffizienten als Matrixelemente aufgefasst werden. Diese Matrizen können mittels der Übergangsraten der Zustände ineinander konstruiert werden. Geht so beispielsweise ein Zustand $|a\rangle$ mit einer Rate $p$ in den Zustand $|b\rangle$ über und $|b\rangle$ mit einer Rate $q$ in $|a\rangle$, hat die zugehörige Matrix $m$ in der Basis $(|a\rangle,|b\rangle)$ die Form
\begin{equation}
m=
\begin{pmatrix}
-p&q\\
p&-q\\
\end{pmatrix}
\end{equation}

Die negativen Einträge stammen daher, dass ein Zustand mit derselben Rate verlassen wird, mit der er in andere übergeht. Daher ist die Summe über alle Einträge einer Spalte einer solchen Matrix stets 0\cite{ASEP_Update}.\\

Dieser Konstruktion folgend erhält man für das ASEP-Modell drei Matrizen. Diese stellen, der Reihe nach, die Vorgänge des Einfügens und Entnehmens eines Teilchens dar, sowie das Hüpfen eines Teilchens auf den benachbarten Platz. Die verwendete Basis ist $(|1\rangle,|0\rangle)$ sowie $(|1\rangle,|0\rangle)\otimes(|1\rangle,|0\rangle)$

\begin{equation}
h_1=
\begin{pmatrix}
0&\alpha\\ 0&-\alpha
\end{pmatrix}
\qquad
h_L=
\begin{pmatrix}
-\beta&0\\ \beta&0
\end{pmatrix}
\end{equation}
\vspace{1px}
\begin{equation*}
h=
\begin{pmatrix}
0&0&0&0\\
0&-1&0&0\\
0&1&0&0\\
0&0&0&0\\
\end{pmatrix}
\end{equation*}

Mit diesen Matrizen kann erneut die zeitliche Änderung des Zustandes beschrieben werden. Dabei ergibt sich

\begin{equation}\label{P_einfach}
\begin{split}
\frac{d}{dt}P_L(\tau_1,\ldots,\tau_L) =& \sum_{\sigma_1}(h_1)_{\tau_1,\sigma_1}P_L(\sigma_1,\tau_2,\ldots,\tau_L)\\
+&\sum_{i=1}^{L-1}\sum_{\sigma_i,\sigma_{i+1}}h_{(\tau_i\tau_{i+1}),(\sigma_i\sigma_{i+1})}P_L(\tau_1,\ldots,\sigma_i,\sigma_{i+1},\ldots,\tau_L)\\
+&\sum_{\sigma_L}(h_L)_{\tau_L,\sigma_L}P_L(\tau_1,\ldots,\tau_{L-1},\sigma_L)
\end{split}
\end{equation}

Wie man durch Einsetzen überprüfen kann, stimmen Gleichungen \ref{P_umständlich} und \ref{P_einfach} überein. Vergleicht man Gleichung \ref{P_einfach} mit Gleichung \ref{MPO_MPS_equ}, so kann man diese als die Wirkung eines Operators $\hat{H}=\hat{h}_1+\hat{h}_L+\sum_{i=1}^L\hat{h}_{i,i+1}$ auf $P_L$ auffassen.\footnote{Dabei ist der Operator $h_{i,i+1}$ der Operator $h$ einwirkend auf Positionen $i$ und $i+1$} Damit vereinfacht sich das Problem weiter zu
\begin{equation}
\frac{d}{dt}P_L=\hat{H}P_L
\end{equation}

Diese lineare Differenzialgleichung lässt sich nun einfach lösen und es ergibt sich

\begin{equation}\label{ASEP_solution_equ}
P_L(t)=e^{\hat{H}t}P_L(0)
\end{equation}

\section{Analytische Lösung}

Das Studium des ASEP-Modells ist unter anderem deshalb sehr nützlich, da ein analytische Lösung existiert, mit der die Ergebnisse einer Näherung verglichen werden können. Aus diesem Grund werden hier die analytischen Lösungen für die mittlere Besetzungszahl an einer beliebigen Position $i$ sowie dem Fluss durch das Gitter im stationären Zustand zusammengefasst, welche in \cite{ASEP} vorgestellt wurden.

Im Folgenden bezeichne $\hat{N}_i$ den Besetzungsoperator an dem Gitterplatz $i$ eines $L$-Positionen ASEP-Modells. Für die mittlere Besetzungszahl $\langle\hat{N}_i\rangle$ gilt dann
\begin{equation}
\langle\hat{N}_i\rangle_L=\sum_{k=0}^{L-i-1}\frac{2k!}{k!(k+1)!}\frac{R(L-k-1)}{R(L)}+\frac{R(i-1)}{R(L)}\sum_{k=2}^{L-i+1}\frac{(k-1)(2(L-i)-k)!}{(L-i)!(L-i-k+1)!}\beta^{-k}
\end{equation}

Die Funktion $R(n)$ ist hierbei gegeben durch\footnote{Die Abhängigikeit von $\alpha$ und $\beta$ wird hier aus Gründen der Übersicht nicht explizit angegeben}
\begin{equation}
R(n)=\sum_{k=1}^{n}\frac{k(2n-1+k)!}{n!(n-k)!}\frac{\beta^{-k-1}-\alpha^{-k-1}}{\beta^{-1}-\alpha^{-1}}
\end{equation}

Dies für große Systeme zu berechnen ist insofern problematisch, da Faktoren bis zu $(2L-2)!$ auftreten. Für große $L$ kann es daher sein, dass auf die asymptotische Lösung für $L\rightarrow\infty$ zurückgegriffen werden muss. Hierbei unterscheidet man zwischen der Lösung an den beiden Rändern des Gitters sowie für Plätze weit entfernt von jenen.

\begin{equation}\label{density_equ}
\langle\hat{N}_i\rangle_L\simeq
\begin{cases}
\frac{1}{2}&\text{für }\alpha\geq\frac{1}{2}\text{ und }\beta\geq\frac{1}{2}\\
\alpha&\text{für }\alpha<\frac{1}{2}\text{ und }\beta\leq\alpha\\
1-\beta&\text{für }\beta<\frac{1}{2}\text{ und }\alpha\leq\beta\\
\alpha+\frac{i}{L}(1-2\alpha)&\text{für }\alpha=\beta<\frac{1}{2}\\
\end{cases}
\end{equation}

Es ist zu sehen, dass die Dichte im Zentrum des Gitters (also für $i/L\simeq 1/2$) im Fall $\alpha<\frac{1}{2}\text{ und }\beta\leq\alpha$ minimal und im Fall $\beta<\frac{1}{2}\text{ und }\alpha\leq\beta$ maximal wird. 

Zur kompakten Beschreibung der Dichten an den Rändern wird zunächst eine neue Größe $\kappa$ eingeführt. Diese ist für große $n$ gegeben durch $\kappa^{-n}=f(\alpha,\beta)R(n)$. Der Faktor $f(\alpha,\beta)$ ist hier nicht relevant, da in den Gleichungen für die Dichten nur Terme der Form $R(a)/R(b)$ vorkommen. Wie in \cite{ASEP} gezeigt, ergibt sich für $\kappa$
\begin{equation}
\kappa=
\begin{cases}
\frac{1}{4}&\text{für }\alpha\geq\frac{1}{2}\text{ und }\beta\geq\frac{1}{2}\\
\alpha(1-\alpha)&\text{für }\alpha\leq\frac{1}{2}\text{ und }\beta>\alpha\\
\beta(1-\beta)&\text{für }\beta\leq\frac{1}{2}\text{ und }\alpha>\beta\\
\end{cases}
\end{equation}

Die erwarteten Dichten am ersten und letzen Gitterplatz lassen sich damit schreiben als
\begin{equation}
\langle\hat{N}_1\rangle_L=1-\frac{\kappa}{\alpha} \qquad \langle\hat{N}_L\rangle_L=\frac{\kappa}{\beta}
\end{equation}

Der Fluss der Teilchen durch das Gitter kann berechnet werden als der Erwartungswert, dass ein Teilchen auf den nächsten Gitterplatz springt. Die Wahrscheinlichkeit, dass dies passiert wurde in Gleichung \ref{rate_equ} bereits verwendet, und ist durch $\tau_i(1-\tau_{i+1})dt$ gegeben. Wie in \cite{ASEP} gezeigt wurde, kann dann der Fluss $J$ zwischen zwei benachbarten Gitterplätzen im stationären Zustand berechnet werden durch
\begin{equation}
J=\langle\hat{N}_i(\hat{\mathbb{1}}-\hat{N}_{i+1})\rangle_L=\frac{R(L-1)}{R(L)}
\end{equation}

Betrachtet man den Grenzfall $L\rightarrow\infty$, ergibt sich daher
\begin{equation}
J=\kappa
\end{equation}

Die drei Fälle, welche für $\kappa$ und damit $J$ unterschieden werden, bilden die Phasen des ASEP-Modells. Wie leicht gesehen werden kann, ist der Fluss im Fall $\alpha\geq\frac{1}{2}\text{ und }\beta\geq\frac{1}{2}$ am größten, man spricht daher von der maximal-Fluss Phase. Die Fälle $\alpha\leq\frac{1}{2}\text{ und }\beta>\alpha$ und $\beta\leq\frac{1}{2}\text{ und }\alpha>\beta$ werden 'niedrige Dichte' und 'hohe Dichte'\todo{bessere Namen?} Phase genannt (vgl. \ref{density_equ}). 


\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{Julia/ASEP-Phasen.png}
\caption{Der Fluss $J$. Die Linien zeigen die Phasengrenzen an}
\end{center}
\end{figure}

 

\chapter{Implementierung}
\section{Implementierung im MPS-Formalismus}

Um das ASEP-Modell mittels des MPS-Formalismus behandeln zu können, ist zunächst eine äquivalente Darstellung eines ASEP-Zustandes nötig. Dabei kann verwendet werden, dass die Wahrscheinlichkeit, einen Zustand $|\psi\rangle$ wie in Gleichung \ref{psi_equ} in einer bestimmten Konfiguration $|\hat{\sigma}_1,\ldots,\hat{\sigma}_L\rangle$ zu finden durch $(c_{\SumIndex})^2$ gegeben ist. Ein ASEP-Zustand kann also geschrieben werden als
\begin{equation}
|\psi\rangle=\sum_{\SumIndexTau} \sqrt{P(\SumIndexTau)}|\SumIndexTau\rangle
\end{equation}
Der Konstruktion aus Abschnitt \ref{construction} folgend kann daraus ein MPS konstruiert werden. Um diese so einfach wie möglich zu halten, wurde der Anfangszustand so vorgegeben, dass alle Positionen unbesetzt sind. Für $P$ gilt daher

\begin{equation}
P(\tau_1,\ldots,\tau_L)=
\begin{cases}
1 & \text{für}\ \tau_1=\ldots=\tau_L=0\\
0 & \text{sonst}
\end{cases}
\end{equation}

Der zu diesem Zustand gehörige MPS ist durch $2L$ Matrizen der Größe $(1\times 1)$ gegeben, für welche gilt $A^{\tau_i}=\delta_{0,\tau_i}$. \par

Die Zeitentwicklung dieser ist nun analog zu der aus Gleichung \ref{ASEP_solution_equ}.\todo{Zeit neu skalieren?} Um den tMPS Algorithmus aus Abschnitt \ref{tMPS} zu verwendet, ist es allerdings nötig, den Hamiltonian als Summe von Nachbar-Wechselwirkungs Hamiltonians zu schreiben. Um dies zu erreichen, kann ausgenutzt werden, dass $\hat{h}_1$ und $\hat{h}_L$ Abkürzungen für die Ausdrücke $\hat{h}_1\otimes\hat{\mathbb{1}}\otimes\ldots\otimes\hat{\mathbb{1}}$ sowie $\hat{\mathbb{1}}\otimes\ldots\otimes\hat{\mathbb{1}}\otimes\hat{h}_L$ sind. Fasst man nun die Tensorprodukte $\hat{h}_1\otimes\hat{\mathbb{1}}$ und $\hat{\mathbb{1}}\otimes\hat{h}_L$ zusammen, und addiert den Operator $\hat{h}_{i,i+1}$ hinzu, erhält man die Nachbar-Wechselwirkungs Hamiltonians
\begin{equation}
\hat{h}_{1,2}^\prime=
\begin{pmatrix}
0&0&\alpha&0\\
0&-1&0&\alpha\\
0&1&-\alpha&0\\
0&0&0&-\alpha\\
\end{pmatrix}
\qquad
\hat{h}_{L-1,L}^\prime=
\begin{pmatrix}
-\beta&0&0&0\\
\beta&-1&0&0\\
0&1&-\beta&0\\
0&0&\beta&0\\
\end{pmatrix}
\end{equation} 

Mit diesen Matrizen kann nun die Zeitentwicklung des Zustandes $|\psi\rangle$ berechnet werden. Um Erwartungswerte zu berechnen ist allerdings nicht nur $|\psi(t)\rangle$, sondern auch $\langle\psi(t)|$ nötig. Löst man die Gleichung \ref{ASEP_solution_equ} erneut für $\langle\psi|$, so findet man
\begin{equation}
\langle\psi(t)|=\langle\psi|e^{\hat{H}t}=(e^{\hat{H}^\dagger t}|\psi\rangle)^\dagger
\end{equation}
Für normale, quantenmechanische Systeme wäre $\hat{H}$ hermitesch, und es gilt daher $\langle\psi(t)|=(|\psi(t)\rangle)^\dagger$. Dies ist hier aufgrund der Asymmetrie des Problems nicht der Fall! Es muss daher, um Erwartungswerte zu berechnen, sowohl $e^{\hat{H} t}|\psi\rangle$ als auch $e^{\hat{H}^\dagger t}|\psi\rangle$ berechnet werden.

Die im ASEP-Modell relevanten Größen sind die Erwartungswerte der Besetzungszahl eines Gitterplatzes sowie des Flusses von einem Platz zum nächsten. Um diese zu berechnen ist der Besetzungs-Operator in der bisher verwendeten Basis nötig. Er ist gegeben durch
\begin{equation}
\hat{N}^{\tau_i,\tau_i^{\prime}}=
\begin{cases}
1&\text{für }\tau_i=\tau_i^{\prime}=1\\
0&\text{sonst}
\end{cases}
\end{equation}

\section{Dichte-Verteilung und Fluss}
Im Folgenden werden die Näherungen des ASEP-Modells durch MPS sowie sMPS präsentiert. Dafür wird zunächst die simulierte Dichte-Verteilung sowie der Fluss mit der exakt berechneten Werten verglichen. 


\subsection{MPS}

\subsection{sMPS}

\section{Transinformation und Entropie}

\chapter{Anhang}
\section{Trotter-Zerlegung}

\section{SVD und NMF}

\bibliographystyle{plain}
\bibliography{Bibliografie} 

%Bessere Konvergenz: Zustand für kleines System berechnen, davon den für großes erraten
\end{document}
